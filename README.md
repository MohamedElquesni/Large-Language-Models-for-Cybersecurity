# üõ°Ô∏è Large Language Models for Cybersecurity (Bachelor Thesis)

> **Bachelor Thesis ‚Äì July 2025**  
> Mohamed Elquesni ¬∑ German University in Cairo (GUC)

---

## üìå Overview

This project investigates the application of **decoder-based Large Language Models (LLMs)** for solving key cybersecurity challenges. While encoder models like BERT have dominated past research, this thesis focuses on instruction-tuned decoder models (e.g., Mistral, LLaMA, Gemma, DeepSeek) and shows their capability in realistic SOC-like tasks.

Experiments were conducted using **Google Colab Pro** with 4-bit quantized models and **QLoRA fine-tuning** via the [Unsloth](https://github.com/unslothai/unsloth) framework.

---

## üîê Implemented Tasks

| Task                            | Model Used                           | Dataset / Source                           | Output Type         |
|---------------------------------|--------------------------------------|--------------------------------------------|---------------------|
| Phishing Email Detection        | Mistral 7B Instruct (Unsloth)        | 5 curated phishing datasets                | Structured JSON     |
| Threat Intelligence Extraction  | LLaMA 3.1 Instruct                   | CTI-HAL Dataset                             | MITRE TTP JSON      |
| Log Anomaly Detection           | Gemma 1.1 Instruct (Unsloth)         | KDDCup‚Äô99                                   | Binary Class Label  |
| Log Query Generation (SPL)      | DeepSeek-Coder 6.7B Instruct         | Custom SPL instruction-template dataset     | SPL Queries         |

---

## üìä Results Snapshot

| Task                      | Metric / Score                         | Notes                                                        |
|--------------------------|----------------------------------------|--------------------------------------------------------------|
| Phishing Detection       | Accuracy: 62.9‚Äì100% across datasets     | High recall, some false positives; JSON valid in 97.8% cases |
| CTI Extraction (CTI-HAL) | F1 Score: 49.3%                         | Outperformed CTI-BERT baseline (F1 47.2%)                    |
| Log Anomaly Detection    | Accuracy: 93.8%, F1 Score: 94.7%        | High precision and recall on KDDCup‚Äô99                       |
| SPL Query Generation     | Template Match: 91%                     | Best results from template-based fine-tuning                 |

---

## üß™ Tools Used

- üß† **LLMs**: Mistral 7B, LLaMA 3.1, Gemma 1.1, DeepSeek-Coder  
- üîß **Fine-tuning**: QLoRA 4-bit using Unsloth  
- ‚òÅÔ∏è **Platform**: Google Colab Pro + A100  
- üß∞ **Libraries**: Hugging Face Transformers, Datasets, PEFT, Accelerate

---

## üì∏ Screenshots & Examples

| Screenshot 
|-----------|
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/1.png) 
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/2.png) 
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/3.png) 
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/4.png) 
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/5.png)
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/6.png) 
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/7.png) 
| ![](https://github.com/MohamedElquesni/Large-Language-Models-for-Cybersecurity/raw/main/Snapshots/8.png) 

---

## üìò Thesis Abstract

This thesis explores the effectiveness of decoder-based LLMs for key cybersecurity tasks. Through fine-tuning and structured prompt design, these models successfully handle phishing email classification, CTI extraction using MITRE ATT&CK alignment, KDD-based anomaly detection, and SPL query generation. The project highlights the potential of decoder architectures for SOC workflows and supports their use in privacy-preserving, local inference setups.

---

## üß† Author

**Mohamed Elquesni**  
German University in Cairo (GUC)  
[LinkedIn](https://www.linkedin.com/in/mohamedelquesni) ¬∑ [Email](mailto:mohamedelquesni@outlook.com)  
Cairo, Egypt  

---

## üèÅ Acknowledgments

Thanks to my advisor, the Unsloth team, and the cybersecurity ML research community.

---
